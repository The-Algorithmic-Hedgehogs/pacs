# Abstract

CRISPR screens are powerful experimental tools used to screen entire genomes in search of genes responsible for [phenotypes](https://en.wikipedia.org/wiki/Phenotype) of interest. With this approach, a single experiment can generate several gigabytes of data that, with sequential implementation, can take many hours to process, limited the depth of sequencing and amount of analysis that can feasibly be performed. Here, we apply principles of parallel computing and algorithm design to expedite the data processing and analysis pipeline significantly. In doing so, we create a framework that provides three important features: i) Facilitation of considerable deeper sequencing experiments through parallelized expedition ii) integration of a sequence distance analysis for improved screening results iii) An associated cost-performance analysis for achieving the desired computational power within given financial constraints. 

# Introduction

The advent of technological advancements such as high-throughput sequencing and genome engineering, along with the increase in available computational power, has allowed biologist to adopt experimental approaches that create millions, sometimes even billions of data points per experiment


# YEET

TO DO 

## Problem Description

TO DO

## Existing Pipeline

TO DO

## Requirement for Big Data and Big Compute


* * *

# Project Design

## Data

TO DO

## Pipeline

TO DO

## Speedup Algorithm

TO DO

# Code Profiling

TO DO

## Infrastructure

TO DO

* * *

# Usage 

TO DO

* * *

# Results

## Performance Evaluation

TO DO

## Optimizations and Overheads

TO DO

* * *

# Discussion

TO DO

# Future Work

* * *

# References

TO DO