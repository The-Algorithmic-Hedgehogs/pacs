{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling sequential code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I profiled the sequential code `count_spacers_with_ED.py` using the `cProfile` Python package. I ran `count_spacers_with_ED.py` with a control file of 100 sequences (*Genome-Pos-3T3-Unsorted_100_seqs.txt*) and an experimental file of 100 sequences (*Genome-Pos-3T3-Bot10_100_seqs.txt*). Each of these input files contained 75 sequencing reads that could be perfectly matched to my database of 80,000 guide sequences, and 25 sequencing reads that needed an edit distance calculation. This breakdown was representative of the proportion of sequencing reads in the full input files that require an edit distance calculation because ~25% of sequencing reads cannot be perfectly matched to one of the 80,000 guide sequences.\n",
    "\n",
    "The exact command I ran was <br>\n",
    "`python -m cProfile -o 100_seq_stats.profile count_spacers_with_ED.py -g ../data/Brie_CRISPR_library_with_controls_FOR_ANALYSIS.csv -u ../data/Genome-Pos-3T3-Unsorted_100_seqs.txt -s ../data/Genome-Pos-3T3-Bot10_100_seqs.txt -o cProfile_test_output`\n",
    " \n",
    "This code was run on my Macbook Pro, which has a 2.2 GHz Intel Core i7 processor with 6 cores.\n",
    "\n",
    "The profiling information was saved in a file called *100_seq_stats.profile*. I will now use the `pstats` package to see what parts of my code are taking the longest and if they can be parallelized.\n",
    "\n",
    "\n",
    "```python\n",
    "print\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats\n",
    "\n",
    "p = pstats.Stats('100_seq_stats.profile'); #read in profiling stats\n",
    "p.strip_dirs(); #remove the extraneous path from all the module names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 29 16:08:45 2019    100_seq_stats.profile\n",
      "\n",
      "         350132307 function calls (350126604 primitive calls) in 537.388 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 1999 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "  3981650  462.315    0.000  534.561    0.000 count_spacers_with_ED.py:35(editDistDP)\n",
      "333437825   69.985    0.000   69.985    0.000 {built-in method builtins.min}\n",
      "  3982102    1.819    0.000    1.819    0.000 {built-in method numpy.zeros}\n",
      "        2    1.414    0.707  535.981  267.990 count_spacers_with_ED.py:69(count_spacers)\n",
      "7992171/7992125    0.447    0.000    0.447    0.000 {built-in method builtins.len}\n",
      "        1    0.215    0.215    0.232    0.232 count_spacers_with_ED.py:7(createDictionaries)\n",
      "    81/79    0.153    0.002    0.156    0.002 {built-in method _imp.create_dynamic}\n",
      "    20674    0.115    0.000    0.365    0.000 stats.py:3055(fisher_exact)\n",
      "      348    0.100    0.000    0.100    0.000 {method 'read' of '_io.FileIO' objects}\n",
      "    41950    0.092    0.000    0.092    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        1    0.075    0.075    0.075    0.075 {method 'dot' of 'numpy.ndarray' objects}\n",
      "      348    0.056    0.000    0.155    0.000 <frozen importlib._bootstrap_external>:830(get_data)\n",
      "    28062    0.052    0.000    0.052    0.000 {built-in method numpy.array}\n",
      "     1604    0.035    0.000    0.035    0.000 {built-in method posix.stat}\n",
      "      348    0.034    0.000    0.034    0.000 {built-in method marshal.loads}\n",
      "    593/1    0.033    0.000  537.389  537.389 {built-in method builtins.exec}\n",
      "        1    0.033    0.033    0.405    0.405 count_spacers_with_ED.py:135(calcGeneEnrich)\n",
      "    81/65    0.024    0.000    0.077    0.001 {built-in method _imp.exec_dynamic}\n",
      "    21078    0.021    0.000    0.071    0.000 fromnumeric.py:69(_wrapreduction)\n",
      "    20675    0.020    0.000    0.020    0.000 {method 'writerow' of '_csv.writer' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1117f3438>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort according to time spent within each function, and then print the statistics for the top 20 functions. \n",
    "p.sort_stats('time').print_stats(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the table above, most of the runtime for our sequential code is spent within the `editDistDP` function. 534 of the 537 seconds, which accounts for 99.4% of our runtime, are spent calculating the edit distance between 50 sequencing reads and 80,000 guides. Generally, the input files contain ~10M sequencing reads, and about 25% of the sequences cannot be matched perfectly to one of the 80,000 guides. Thus for two input files of ~10M sequencing reads (~20M reads total), there are ~4-5M sequencing reads for which the edit distance calculations must be performed. If this code was run sequentially, this would require 10,000 hours of runtime. Therefore, we need to parallelize this portion of the code.\n",
    "\n",
    "The edit distance calculation is currently nested within the function `count_spacers`, which matches each sequencing read from the input files to one of the 80,000 guides. For 200 sequencing reads provided as input, 1.4 seconds are spent performing the matching, which is only 0.007 seconds per sequencing read (I am using the 1.4 seconds from the *tottime* column because the *cumtime* takes into account the edit distance calculation). This number grows large if we have 20M sequencing reads we need to match because it would take $\\dfrac{0.007\\text{seconds/read} \\cdot 20\\text{M reads}}{3600\\text{seconds/hour}} = 39\\text{hours}$. Thus, the entire matching process of our workflow needs to be parallelized.\n",
    "\n",
    "We want to parallelize this matching process by using a Spark cluster to have access to as many cores as possible to perform both the matching process and edit distance calculation (if needed). We will partition each input file into many tasks, and each task will run on a single core of the Spark cluster so a single core will perform both the matching process and edit distance for the sequencing reads in a partition. From what we have determined, there is not an easy way to parallelize the actual edit distance calculation between two strings. However, for a given sequencing read, we should be able to parallelize the 80,000 edit distance calculations that need to be performed between the sequencing read and the 80,000 guides by using Python multi-threading or possibly a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overheads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not know which sequences from our input files we will need to perform edit distance calculations for, load-balancing is the main overhead we anticipate dealing with because we do not want one or two cores slowed down with having to compute too many edit distance calculations. We would like to spread the number of edit distance calculations out evenly between the cores by tuning the number of Spark tasks. It may be good to shuffle the order of the sequencing reads because sometimes many sequences that require edit distance calculations are adjacent to each other in the input file.\n",
    "\n",
    "If we try to use a GPU to perform the 80,000 edit distance calculations in parallel, memory-transfer (input/output) to the GPU would also be an overhead. For a single sequencing read, multiple transfers would need to be performed because we would not be able to perform the 80,000 calculations in parallel because we are limited by the number of cores on the GPU. Currently, we do not have a good way of mitigating this overhead if we were to use GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence matching and edit distance portion of our code accounts for 99.4% of the runtime in our small example. With larger problem sizes, this percentage should only increase because the number of operations performed after the sequence matching and edit distance section is constant.\n",
    "\n",
    "Amdahl's Law states that potential program speedup $S_t$ is defined by the fraction of code $c$ that can be parallelized, according to the formula\n",
    "$$\n",
    "S_t = \\dfrac{1}{(1-c)+\\frac{c}{p}}\n",
    "$$\n",
    "where $p$ is the number of processors/cores. In our case, $c=0.994$ and the table below shows the speed-ups for $2$, $4$, $8$, $64$, and $128$ processors/cores:\n",
    "\n",
    "|processors|speed-up|\n",
    "|----------|--------|\n",
    "|2|1.98x|\n",
    "|4|3.93x|\n",
    "|8|7.68x|\n",
    "|64|46.44x|\n",
    "|128|72.64x|\n",
    "\n",
    "Thus, our strong-scaling is almost linear when $p$ is small, but we observe that this begins to break down because we only get 73x speed-up if we were to use 128 processors/cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gustafson's Law states larger systems should be used to solve larger problems because there should ideally be a fixed amount of parallel work per processor. The speed-up $S_t$ is calculated by\n",
    "$$\n",
    "S_t = 1 - c +c\\cdot p\n",
    "$$\n",
    "where $p$ is the number of processors/cores. In our case, $c=0.994$ and the table below shows the speed-ups for $2$, $4$, $8$, $64$, and $128$ processors/cores:\n",
    "\n",
    "|processors|speed-up|\n",
    "|----------|--------|\n",
    "|2|1.994x|\n",
    "|4|3.98x|\n",
    "|8|7.96x|\n",
    "|64|63.62x|\n",
    "|128|127.23x|\n",
    "\n",
    "Thus, we almost achieve perfect weak-scaling because we can split up larger problem-sizes (which would be larger input files in our case) over more processors to achieve about the same runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
